<!DOCTYPE html>
<html>
  <head>
    <base target="_top">
  </head>
  <body>
    

<h1>Example framework for audio-to-text with/without Q&A on text</h1> 
<label id="directions" style="display:block">Enter OpenAI key to transcribe the audio files to text. All audio files in the repository will be transcribed.</label>
	  
<br>

<!-- ----------------------------------- -->
<input id="OPENAI_API_KEY" type="text" value="" placeholder="OPENAI_API_KEY" rows="10" cols="100" style="display:block; text-align: left; width: 150px;">
<!-- ----------------------------------- -->
	
<br><br>

<button id="transcribe_audio_files" onclick="transcribe_audio_files()">transcribe_audio_files</button>
<br><br>

<button id="fine_tune_text" onclick="fine_tune_text()">fine_tune_text</button>

<!-- <audio controls id="audio_id" style="display:none"><source src="2024_03_13_09_58_13_test1.mp3" type="audio/mpeg"></audio> -->
	  
	  
<!-- View text results -->
<div id="progress" style="display:block;font-family:courier;font-size:24px;height:300px"></div>
<div id="transcriptions" style="display:none;font-family:courier;font-size:24px;height:300px"></div>

<!-- View transcribed_text and chatbot -->
<div align="left">
    <table style='text-align: center; width: 300px; display:block'>
      <tr>
        <td>
		<label id="transcribed_text_label" style="display:block">Transcribed text</label>
      		<br>
      		<textarea id="transcribed_text" rows="35" cols="100" placeholder="Transcribed text" style="display:block"></textarea>
	</td>
        <td>
		<label id="chatbot_label" style="display:block">Chatbot area</label>
      		<br>
      		<textarea id="chatbot" rows="35" cols="100" placeholder="Chatbot text" style="display:block"></textarea>
	</td>
      </tr>
    </table>
</div>  
    



	  
<!-- --------------------------------------------------- -->

<!-- CSS -->
<style>
div {position: relative; z-index: 2;},
  table {border-collapse: collapse; position: absolute;}
  td,
  th {border: 1px solid black;padding: 10px 20px;}
audio {position: absolute; top: 350px;}
</style>

	  
<!-- --------------------------------------------------- -->
	  


	  
<script>

const outp = document.getElementById('progress');
const outt = document.getElementById('transcriptions');

const repoOwner = 'CodeSolutions2';
const repoName = 'audio_2_text_webapp';

const OPENAI_API_KEY = document.getElementById("OPENAI_API_KEY").value;

var month = {"01": "january", "02": "february", "03": "march", "04": "april", "05": "may", "06": "june", "07": "july", "08": "august", "09": "september", "10": "october", "11": "november", "12": "december"};

// ----------------------------------------------------

async function play_an_audio() {
	
	// https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio#controls
	// document.getElementById("audio_id").style.display = "block";
}

	
// ----------------------------------------------------

async function fine_tune_text() {

  var transcribed_text = outt.innerHTML;
  document.getElementById("transcribed_text").innerHTML = transcribed_text;

  // Sorting
	const regexp = /[0-9{4}]+_[0-9{2}]+_[0-9{2}]+_[0-9{2}]+_[0-9{2}]+_[0-9{2}]+\n/g; // pattern to look for

	let matches = [...transcribed_text.matchAll(regexp)];
	// console.log('matches: ', matches);
	
	let text_st = 0;
	let text_end = 0;
	let text_by_date = [];
	
	// next: get array of start and end values to cut string
	matches.forEach(async function(row, index) {
		date = row.at(0);
		// console.log('date: ', date);
		text_st = date.length + text_end;
			
		if (matches.length > (index+1)){
			text_end = matches[index+1].index - 1;
		} else {
			text_end = transcribed_text.length;
		}
		// console.log('text_st: ', text_st, 'text_end: ', text_end);
		text_by_date.push({date: date, text: transcribed_text.slice(text_st, text_end)});
	});
	console.log('text_by_date: ', text_by_date);

	// -----------------------------------
	
	// Optional: could sort the Array of dictionaries by date, so it is in chronological order

	// -----------------------------------
	
	// Creation of Questions and Answers from text: create 10 questions and answers about each text
	let num_of_QA_pairs = 5;
	var Q = [];
	var A = [];
	text_by_date.forEach(async function(row, index) {
		let year = row.date.slice(0,4);
		console.log('year: ', year);
		
		let month_number = row.date.slice(5,7); 
		console.log('month_number: ', month_number);
		
		let month_name = month[month_number];
		console.log('month_name: ', month_name);
		
		// const prompt = `Create ${num_of_QA_pairs} questions about the given text: ${row.text}, and then answer the created questions using the given text. Each question should begin with: 'For ${month_name} ${year},'.`;
		// Creates questions only
		
		const prompt = `Create ${num_of_QA_pairs} questions and answers about the given text: ${row.text}. Each question should begin with: 'For ${month_name} ${year},' and each answer should begin with 'A:'.`;
		  // ------------------------------------------
		  var system_content = "You are a helpful assistant"
		  var assistant_content = "Respond concisely"
		  var messages = [{"role": "system", "content": system_content}, {"role": "user", "content": prompt}, {"role": "assistant", "content": assistant_content}];
		  var data = {"model": 'gpt-3.5-turbo', 'messages': messages, 'temperature': 0};
		  var url = 'https://api.openai.com/v1/chat/completions'
		  var headers = {"Content-Type": "application/json", "Authorization": 'Bearer ' + OPENAI_API_KEY}
		  var options = {method : 'post', headers: headers, body : JSON.stringify(data)};
		  
		  // Print text part of JSON response only with user message
		  var QA_pairs = await fetch(url, options)
		            .then(res => res.json())
		            .then(res => { const QA = JSON.parse(JSON.stringify(res))['choices'][0]['message']['content'];
		                          console.log('QA: ', QA); 
		                          return QA;
		            })
		            .catch(error => { console.log(error); });

	});
	
	// -----------------------------------

	// Prepare to fine tune

	// system_content = "You are a helpful personal assistant. You answer questions about events and ideas for a specific month and year.";


// 
  
			// const jsonl_data = JSON.stringify(data, null, 2);
			// console.log('jsonl_data: ', jsonl_data);
			
			// return new Blob([jsonl_data], { type: 'application/json' });
		
		//.then(async function(blob_object) { await openai_upload_file_for_finetuning(blob_object, 'blob_object') })

}

// ----------------------------------------------------

async function openai_upload_file_for_finetuning(file_input, which_input) {

	const url = "https://api.openai.com/v1/files";

	const headers = new Headers();
	headers.append("Authorization", 'Bearer ' + OPENAI_API_KEY);
	headers.append("Accept", "application/json");
	
	const formData = new FormData();
	if (which_input == 'blob_object') {
		formData.append("file", file_input);
	} else {
		// using a file_blob_object
		formData.append("file", file_input, "file.json");
	}
	formData.append("purpose", "fine-tune");
	
	const options = {method: 'POST', 
		       headers: headers, 
		       body: formData,
		       redirect: "follow"
		      };
	
	await fetch(url, options)
		.then(response => response.text())
		.then(async function(result) { 
			console.log(result);
		})
		.catch(error => { console.log(error); });
	
}
	
// ----------------------------------------------------


async function transcribe_audio_files() {
	
	await play_an_audio()
		.then(async function() { 
			// Processing information
			outp.innerHTML = 'Processing audio';
			// readaudio values are delayed becuse of processing with OpenAI endpoint
			return await readaudio(); 
		})
		// Return a promise to force values created after readaudio to be created in sychronous order.
		// Assure that readaudio is finished : ok so readaudio returns a little after the callback
		.then(done_message => done_message)
		.then(done_message => { outp.innerHTML = done_message; })

	// Eventhough there is a promise, readaudio values return seconds after transcribe_audio_files has finished. 
}


// ----------------------------------------------------


async function readaudio() {

	return await get_audio_file_objects()
	.then(file_objects => { 
		
		// console.log("file_objects:", file_objects);
    
		file_objects.forEach(async function(file_object, index) {
			
			console.log("file_object:", file_object);
			
			filename = file_object.name;
			// console.log("filename:", filename);
			
			let out0 = filename.split('_');

			// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date
			let YYYY = out0.slice(0,1).toString();
			let MM = out0.slice(1,2).toString();
			let DD = out0.slice(2,3).toString();
			let HH = out0.slice(3,4).toString();
			let mm = out0.slice(4,5).toString();
			let ss = out0.slice(5,6).toString();

			const regex = /(,)+/g;
			
			YYYY = YYYY.replace(regex, '');
			MM = MM.replace(regex, '');
			DD = DD.replace(regex, '');
			HH = HH.replace(regex, '');
			mm = mm.replace(regex, '');
			ss = ss.replace(regex, '');

			let date_string = `${YYYY}_${MM}_${DD}_${HH}_${mm}_${ss}`;

			// -----------------------------------

			let file_download_url = file_object.download_url;
			// console.log("file_download_url:", file_download_url);
			
			// -----------------------------------

			// REST API to convert .mp3 to text
			
			// ------------------------------------------
			// Form submission
			// ------------------------------------------
			// Way 2: read in the mp3 as file_download_url --> convert to  blob_object
			await fetch(file_download_url)
  				.then(response => response.blob())
			 	.then(async function(blob_object) { 
					// console.log("blob_object: ", blob_object);
					await openai_transcription(blob_object, 'blob_object', date_string);
					// OR
					// return await openai_translation(blob_object, 'blob_object');
				})
				.then(async function() { await new Promise(r => setTimeout(r, 200)); })
				.catch(error => { console.log(error); });

			// ------------------------------------------
			
		  });  // end of file_objects.forEach
	})
	.then(() => { return 'Transcription Processing Done'; });
	
}


// ----------------------------------------------------


async function openai_transcription(file_input, which_input, date_string) {

	const url = 'https://api.openai.com/v1/audio/transcriptions';

	const headers = new Headers();
	headers.append("Authorization", 'Bearer ' + OPENAI_API_KEY);
	headers.append("Accept", "application/json");
	
	const formData = new FormData();
	if (which_input == 'blob_object') {
		formData.append("file", file_input);
	} else {
		// using a file_blob_object
		formData.append("file", file_input, "recording.mp3"); // mp3, mp4, mpeg, mpga, m4a, wav, or webm
	}
	formData.append("model", "whisper-1");
	formData.append("prompt", "Transcribe the audio");  // Style context for the transcription
	formData.append("response_format", "text");  // json, text, srt, verbose_json, or vtt
	formData.append("temperature", "0");  // The sampling temperature, between 0 (accurate) and 1 (random response)
	formData.append("language", "en");
	formData.append("transcription", "plain text"); // plain text, srt, vtt
	
	const options = {method: 'POST', 
		       headers: headers, 
		       body: formData,
		       redirect: "follow"
		      };
	
	// Print text part of JSON response only with user message
	await fetch(url, options)
		.then(response => response.text())
		.then(async function(result) { outt.innerHTML += `${date_string}\n${result}\n`; })
		.catch(error => { console.log(error); });
	
}
	

// ----------------------------------------------------


async function openai_translation(file_input) {
	
	const url = 'https://api.openai.com/v1/audio/translations';

	const headers = new Headers();
	headers.append("Authorization", 'Bearer ' + OPENAI_API_KEY);
	headers.append("Accept", "application/json");
	
	const formData = new FormData();
	if (which_input == 'blob_object') {
		formData.append("file", file_input);
	} else {
		// using a file_blob_object
		formData.append("file", file_input, "recording.mp3"); // mp3, mp4, mpeg, mpga, m4a, wav, or webm
	}
	formData.append("model", "whisper-1");
	formData.append("prompt", "Transcribe the audio");  // Style context for the transcription
	formData.append("response_format", "text");  // json, text, srt, verbose_json, or vtt
	formData.append("temperature", "0");  // The sampling temperature, between 0 (accurate) and 1 (random response)
	formData.append("language", "en");
	formData.append("transcription", "plain text"); // plain text, srt, vtt
	
	const options = {method: 'POST', 
		       headers: headers, 
		       body: formData,
		       redirect: "follow"
		      };
	
	// Print text part of JSON response only with user message
	return await fetch(url, options)
		.then(response => response.text())
		.then(result => { console.log(result); return result; })
		.catch(error => { console.log(error); });
	
}

  
// ----------------------------------------------------

	
async function get_audio_file_objects() {
	
	var url = `https://api.github.com/repos/${repoOwner}/${repoName}/contents`;

	var file_objects = [];
	await fetch(url).then(res => res.json()).then(data => {
		    data.forEach(file => {
		      if (file.type === 'file' && file.name.match(/.(mp3)$/i)) {
            file_objects.push(file);
		      }
		    });
		  }).catch(error => { outp.innerHTML += error; });
	
    	console.log('file_objects: ', file_objects);
	
	return file_objects;
}

// ----------------------------------------------------
  
</script>

  </body>
</html>
